aci_exporter:
  image:
    repository: quay.io/camillo/aci-exporter
    tag: v0.8.0
    pullPolicy: Always
  port: 9643
  # IF you change this URL YOU MUST Change the aciServiceDiscoveryURLs.sd value to match
  url: aci-exporter-svc
  httpclient:
    insecurehttps: true
    keepalive: 120
    timeout: 30
    pagesize: 1000
  fabrics: {}
  
  # Helm can't use variables in a Value file so naming the service 
  # Dynamically (i.e. including the release name) will break the prometheus sub-chart
  # The easiest solution (and the only one I found that does not involves patching the prom sub-chart)
  # is to use anchors so we can define the http_service_discovery URL needed by prometheus 
  # and then pass it in the prometheus.serverFiles.prometheus.yml.scrape_configs.http_sd_configs.url paramater
  # Sadly acnhors don't support string concatenation so we have to ensure the aci-sd-url matches the config.port and config.url
  # You  need to change this ONLY if you want to deploy this chart multiple times in the same namespace to avoid 
  # having duplicate service name in the same NS.
  # Note: The url is then expanded in the config map as {{ $v.url }}.{{ $.Release.Namespace }}.svc:{{ $.Values.aci_exporter.port }}/sd
  # so do not include ports or paths here.
  aciServiceDiscoveryURLs:
    sd:
      url: http://aci-exporter-svc
      apic_polling: 5m
      apic_scrape_timeout: 4m
      switch_polling: 1m
      switch_scrape_timeout: 30s
  prefix: aci_

prometheus:
  configmapReload:
    prometheus:
      extraConfigmapMounts:
        - name: prometheus-alerts
          mountPath: /etc/alerts.d
          configMap: prometheus-alerts
          readOnly: true
      extraVolumeDirs:
        - /etc/alerts.d
  # We are deploying a dedicated Prometheus instance for the aci-exporter
  # So we don't care about collecting metrics from the K8s cluster itself
  kube-state-metrics:
    enabled: false
  prometheus-node-exporter:
    enabled: false
  prometheus-pushgateway:
    enabled: false
  server:
    configMapOverrideName: prometheus-config
    extraConfigmapMounts:
      - name: prometheus-alerts
        mountPath: /etc/alerts.d
        configMap: prometheus-alerts
        readOnly: true
    prefixURL: ""
    extraArgs:
      # Is a bit of an odd syntax but this is how is ENABLED
      web.enable-remote-write-receiver: null

  service:
    servicePort: 80
  alertmanager:
    enabled: true
    service:
      port: 9093
    templates:
      alertmanager-webex-template.tmpl: |-
        {{/* Title of the Webex alert */}}
        {{ define "webex.default.message" -}}
        [{{ .Status | toUpper -}}
        {{ if eq .Status "firing" }}:{{ .Alerts.Firing | len }}{{- end -}}
        ] {{ .CommonLabels.alertname }}
        {{ template "Webex.text" . }}
        {{- end }}

        {{/* Severity of the alert */}}
        {{ define "__alert_severity" -}}
            {{- if eq .CommonLabels.severity "critical" -}}
            *Severity:* `Critical`
            {{- else if eq .CommonLabels.severity "warning" -}}
            *Severity:* `Warning`
            {{- else if eq .CommonLabels.severity "info" -}}
            *Severity:* `Info`
            {{- else -}}
            *Severity:* `undefined` {{ .CommonLabels.severity }}
            {{- end }}
        {{- end }}

        {{/* The text to display in the alert */}}
        {{ define "Webex.text" -}}

            {{ template "__alert_severity" . }}
            {{- if (index .Alerts 0).Annotations.summary }}
            {{- "\n" -}}
            *Summary:* {{ (index .Alerts 0).Annotations.summary }}
            {{- end }}

            {{ range .Alerts }}

                {{- if .Annotations.description }}
                {{- "\n" -}}
                {{ .Annotations.description }}
                {{- "\n" -}}
                {{- end }}
                {{- if .Annotations.message }}
                {{- "\n" -}}
                {{ .Annotations.message }}
                {{- "\n" -}}
                {{- end }}

            {{- end }}
        {{- end }}

grafana:
  enabled: true
  defaultDashboardsEnabled: false
  sidecar:
   # This will load datasorces that are defined as ConfigMaps with the label grafana_datasource
    datasources:
      name: Prometheus
      uid: prometheus
      enabled: true
      defaultDatasourceEnabled: true
      isDefaultDatasource: true
      label: grafana_datasource
      labelValue: "aci-monitoring-stack"
      alertmanager:
        enabled: true
        name: Alertmanager
        uid: alertmanager
        handleGrafanaManagedAlerts: false
        implementation: prometheus
    dashboards:
      enabled: true
      label: grafana_dashboard
      labelValue: "aci-monitoring-stack"
      # This is needed to place the dashboard inside the folder specified
      # in the `k8s-sidecar-target-directory` annotation in the ConfigMap
      provider:
        foldersFromFilesStructure: true
        allowUiUpdates: true
    alerts:
      enabled: false
    rbac:
      create: false
promtail:
  enabled: true
  config:
    clients:
    - url: http://loki-gateway/loki/api/v1/push
    snippets:
      scrapeConfigs: |
        {{- range $k, $v := .Values.extraPorts }}
        - job_name: {{ $v.name }}
          syslog:
            listen_address: 0.0.0.0:{{ $v.containerPort }}
            labels:
              job: aci-monitoring-stack
              fabric: {{ $v.name }}
          relabel_configs:
            - source_labels:
                - __syslog_message_hostname
              target_label: switch
          pipeline_stages:
            # ACI log levels don't map to the Grafana hard coded one so to get coloring right I just map the ACI levels to Grafana ones.
            - regex: 
                expression: '\[(?P<level>critical|major|minor|warning|cleared|info)\]'
            - replace:
                expression: '^(major)$'
                source: level
                replace: 'error'
            - replace:
                expression: '^(minor)$'
                source: level
                replace: 'warning'
            - replace:
                expression: '^(cleared)$'
                source: level
                replace: 'info'
            - labels:
                level:
          {{- end }}
  daemonset:
    enabled: false
  deployment:
    enabled: true
  replicaCount: 1
  #Only used to ingest external logs from ACI so no need to mount any of the local volumes
  defaultVolumes: []
  defaultVolumeMounts: []
  podSecurityPolicy:
    privileged: fasle
    allowPrivilegeEscalation: false
    volumes:
      - 'secret'
      - 'downwardAPI'
syslog:
  enabled: false
  image:
    repository: lscr.io/linuxserver/syslog-ng
    tag: 4.8.1
    pullPolicy: IfNotPresent

loki:
  enabled: true
  openshift-buckets:
    enabled: false
    bucketName: loki
    storageClassName: ocs-storagecluster-ceph-rgw
  fullnameOverride: "loki"
  monitoring:
    dashboards:
      enabled: false
    rules:
      enabled: false
    serviceMonitor:
      enabled: false
    selfMonitoring:
      enabled: false
      grafanaAgent:
        installOperator: false
    lokiCanary:
      enabled: false
  lokiCanary:
    enabled: false
  test:
    enabled: false
  loki:
    enabled: true
    auth_enabled: false
    gateway:
      service:
        port: 80
    datasource:
      jsonData: "{}"
      uid: ""
    commonConfig:
      replication_factor: 1
    limits_config:
      discover_log_levels: false
      discover_service_name: []
    schemaConfig:
      configs:
        - from: 2024-04-01
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: loki_index_
            period: 24h
    ingester:
      chunk_encoding: snappy
    tracing:
      enabled: false
    querier:
      # Default is 4, if you have enough memory and CPU you can increase, reduce if OOMing
      max_concurrent: 4
    sidecar:
      rules:
        enabled: true
        label: loki_rule
        labelValue: ""
    rulerConfig:
      wal:
        dir: /rules/ruler-wal
      storage:
        type: local
        local:
          directory: /rules
      rule_path: /rules/fake
      remote_write:
        enabled: true
        clients:
          fake:
            url: http://aci-mon-stack-prometheus-server.{{ .Release.Namespace }}.svc:80/api/v1/write
      alertmanager_url: http://aci-mon-stack-alertmanager.{{ .Release.Namespace }}.svc:9093
      ring:
        kvstore:
          store: inmemory
      enable_alertmanager_v2: true
  deploymentMode: SimpleScalable
  backend:
    replicas: 3
    persistence:
      size: 2Gi
  read:
    replicas: 3
  write:
    replicas: 3
    persistence:
      size: 2Gi
  # Enable minio for storage
  minio:
    enabled: true
  singleBinary:
    replicas: 0
  chunksCache:
    allocatedMemory: 1024
  ingester:
    replicas: 0
  querier:
    replicas: 0
  queryFrontend:
    replicas: 0
  queryScheduler:
    replicas: 0
  distributor:
    replicas: 0
  compactor:
    replicas: 0
  indexGateway:
    replicas: 0
  bloomCompactor:
    replicas: 0
  bloomGateway:
    replicas: 0

backup2graph:
  enabled: true
  # How often to run the backup2graph job this is a cron schedule expressions by default is every 15min
  schedule: "*/15 * * * *"
  nodeSelector: {}
  image:
    repository: quay.io/datacenter/backup2graph
    tag: v0.1.4
    pullPolicy: Always
memgraph:
  enabled: true
  boltPort: 7687
  container:
    terminationGracePeriodSeconds: 60
  sysctlInitContainer:
    enabled: false
  persistentVolumeClaim:
    createUserClaim: true
    userStorageAccessMode: "ReadWriteMany"
    # DO NOT CHANGE: backup2graph is hardcoded to use this path
    userMountPath: /app/fabrics/